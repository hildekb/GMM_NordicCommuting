{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d2da8f-19a0-4526-b7bb-414557bccdc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "#from shapely.geometry import shape\n",
    "import utils\n",
    "import importlib\n",
    "\n",
    "#from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e646a7",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\hildlars\\\\OneDrive - Universitetet i Oslo\\\\NordicMathCovid\\\\PanNordicMobility\\\\FinalFiles_commutingPaper\\\\Code\\\\utils.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b0074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_SWEDEN_csv(SWE_file):\n",
    "    SWE_data = pd.read_csv(SWE_file, parse_dates=[\"DateTimeFromUtc\", \"DateTimeToUtc\"], delimiter=\";\")\n",
    "\n",
    "\n",
    "    SWE_data.loc[SWE_data['DetectorId'] == '53VIP0008-1','DetectorId'] = '53VIP0008_A'\n",
    "    SWE_data.loc[SWE_data['DetectorId'] == '53VIP0008-2', 'DetectorId'] = '53VIP0008_A'\n",
    "    SWE_data.loc[SWE_data['DetectorId'] == '53VIP0008-3','DetectorId'] = '53VIP0008_B'\n",
    "    SWE_data.loc[SWE_data['DetectorId'] == '53VIP0008-4', 'DetectorId'] = '53VIP0008_B'\n",
    "\n",
    "    # Sum over sensor\n",
    "    SWE_data = SWE_data[['DetectorId', 'DateTimeFromUtc', 'FlowVpm']].groupby(['DetectorId', 'DateTimeFromUtc']).sum().reset_index()\n",
    "    \n",
    "    \n",
    "    # Date and time\n",
    "    SWE_data['date'] = SWE_data['DateTimeFromUtc'].apply(lambda x: x.date())\n",
    "    SWE_data['date'] = pd.to_datetime(SWE_data['date'])\n",
    "    SWE_data['hour'] = SWE_data['DateTimeFromUtc'].apply(lambda x: x.hour)\n",
    "    SWE_data['minute'] = SWE_data['DateTimeFromUtc'].apply(lambda x: x.minute)\n",
    "    \n",
    "    \n",
    "    # Select the columns needed\n",
    "    SWE_data = SWE_data[['DetectorId', 'date', 'hour','minute', 'FlowVpm']]\n",
    "\n",
    "\n",
    "    # Merge origin and destination country with data\n",
    "    SensorCountry = pd.DataFrame(\n",
    "        [[\"53VIP0008_A\", \"DEN\", 'SWE','1'], ['53VIP0008_B', 'SWE', 'DEN', '2']], columns=[\"DetectorId\", \"origin_country\", \"dest_country\", 'sensor_dir'])\n",
    "    SWE_data = pd.merge(SWE_data, SensorCountry, on='DetectorId')\n",
    "    SWE_data.loc[SWE_data['DetectorId'] == '53VIP0008_A','DetectorId'] = '53VIP0008'\n",
    "    SWE_data.loc[SWE_data['DetectorId'] == '53VIP0008_B', 'DetectorId'] = '53VIP0008'\n",
    "\n",
    "    # Rename and select columns\n",
    "    SWE_data = SWE_data.rename(columns={'DetectorId': 'sensor_id', 'FlowVpm': 'total_vehicles'})\n",
    "    return(SWE_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a518567",
   "metadata": {},
   "source": [
    "Read data from .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8105e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWE_file = \"../Data/DenSwe/Ã–resundsbron_2022-04-21_2.csv\"\n",
    "data = read_SWEDEN_csv(SWE_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abaebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_data_DENSWE(data,min_date = datetime.datetime(2019, 1, 1)  , max_date = datetime.datetime(2021, 12, 31)):\n",
    "\n",
    "    agg_data = data.copy()\n",
    "\n",
    "    ##Create a new variables that contain both sensor name and country name\n",
    "    agg_data[\"sensor_id\"] = agg_data[\"sensor_id\"].astype(str)\n",
    "    agg_data['sensor_origin'] = agg_data[['sensor_id', 'origin_country']].agg(', '.join, axis=1)\n",
    "    agg_data['sensor_destination'] = agg_data[['sensor_id', 'dest_country']].agg(', '.join, axis=1)\n",
    "\n",
    "\n",
    "    agg_data[\"minute\"] = pd.to_timedelta(agg_data[\"minute\"], unit=\"min\")\n",
    "    agg_data[\"hour\"] = pd.to_timedelta(agg_data[\"hour\"], unit=\"h\")\n",
    "\n",
    "\n",
    "    agg_data['date'] = agg_data['date'] + agg_data['hour'] + agg_data['minute']\n",
    "    agg_data = agg_data[(agg_data.date > min_date) & (agg_data.date < max_date)].copy().reset_index()\n",
    "    \n",
    "\n",
    "    min_date = agg_data.date.min()\n",
    "    max_date = agg_data.date.max()\n",
    "\n",
    "    agg_data = agg_data.groupby(['sensor_origin', 'sensor_destination', 'date'])['total_vehicles'].sum().reset_index().copy()\n",
    "    rm_idx = np.where(np.isnan(agg_data.total_vehicles))\n",
    "    agg_data = agg_data.drop(index = rm_idx[0])\n",
    "    agg_data['total_vehicles'] = agg_data['total_vehicles'].apply(lambda x: int(x))\n",
    "\n",
    "    f = lambda x: x.reindex(pd.date_range(min_date,\n",
    "                                            max_date,\n",
    "                                            name='date',\n",
    "                                            freq='1min'), fill_value=0)\n",
    "\n",
    "\n",
    "    agg_data = (agg_data.set_index('date')\n",
    "                .groupby([\"sensor_origin\", \"sensor_destination\"])[\"total_vehicles\"]\n",
    "                .apply(f)\n",
    "                .reset_index())\n",
    "    agg_data = agg_data.pivot_table(index=[\"sensor_origin\",\"sensor_destination\"], columns=[\"date\"],values=[\"total_vehicles\"] ).droplevel(level = 0,axis = 1 )\n",
    "\n",
    "    return(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5778f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_date = datetime.datetime(2019, 1, 1)\n",
    "#max_date = datetime.datetime(2021, 12, 31)\n",
    "max_date = datetime.datetime.combine(date = data['date'].max().date(), time = datetime.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55c2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2017-2023 this takes about 4 min to run\n",
    "agg_data = agg_data_DENSWE(data, min_date = min_date  , max_date = max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f03e9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n"
     ]
    }
   ],
   "source": [
    "# This takes about 3 min to run\n",
    "models_pre, data_pre = utils.fit_period(agg_data, d1=datetime.date(2019,1,1), d2=datetime.date(2020,3,1), hourly = False, nSamp = 10000, Normalize = False, N = 10, seed=2024, FitMethod = 'Bayesian')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513741ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the fitted model and data\n",
    "models_pre.to_pickle('../Data/models_den.pkl')\n",
    "data_pre.to_pickle('../Data/data_den.pkl')\n",
    "agg_data.to_pickle('../Data/agg_data_den.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "142fce53b6f2b91b97dca80dfeb7f3677964d22b620fffd446cffe90630072c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

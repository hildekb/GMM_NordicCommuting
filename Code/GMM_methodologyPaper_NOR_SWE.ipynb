{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d2da8f-19a0-4526-b7bb-414557bccdc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import requests\n",
    "#import json\n",
    "#import zipfile\n",
    "import datetime\n",
    "import time\n",
    "#from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import shape\n",
    "import utils\n",
    "import importlib\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import nimfa\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e646a7",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\hildlars\\\\OneDrive - Universitetet i Oslo\\\\NordicMathCovid\\\\GitProjects\\\\NordicMobilityData_WorkingRepo\\\\PythonCode\\\\GaussianMixture\\\\utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b141e071-f28d-4a29-b506-ae3d68486819",
   "metadata": {},
   "outputs": [],
   "source": [
    "FolderPath = '../Data/NorSwe/' # Path to raw data files.\n",
    "\n",
    "Sensors = [\"01777V885181\", # OK\n",
    "\"77275V885276\", # OK\n",
    "#\"51812V1203972\", # Missing: a bit 2018 and early 2019-late 2020. Wount capture pandemic change with this\n",
    "\"35829V885266\", # Missing: 2017-2018\n",
    "#\"08581V885541\", # Missing: 2017-mid 2019. Too little data for model fit, but include later? Not a lot of data, skip this one.\n",
    "#\"98823V578220\", # Missing: 2017-mid 2019 and early 2020 and a bit 2022    Røyrvik, litt tvilsom\n",
    "\"99923V578123\", # OK\n",
    "#\"93561V578187\", # Missing: 2017-mid 2019 and a bit late 2019. Cannot compare before and after pandemic\n",
    "\"50089V578151\", # Missing: 2017-late 2018 (+ start of pandemic?). ?????????\n",
    "\"84237V578097\", # OK\n",
    "#\"11051V704737\", # Missing: partly 2017 and 2021 - end. Drevsjø øst, litt tvilsom. Ikke post-pandemic data\n",
    "\"69140V704643\", # OK\n",
    "#\"14158V705081\", # Missing: 2017-2019 + sporadisk. Flermoberget, litt tvilsom ??????????\n",
    "\"00737V704646\", # OK\n",
    "\"94864V704707\", # Missing: sporadisk gjennom pandemien ????????\n",
    "\"94299V704696\", # OK\n",
    "\"57929V705247\", # Missing: A bit 2018/1019     Øyermoen, litt tvilsom\n",
    "\"76778V704564\", # OK  Morokulien, litt tvilsom\n",
    "\"05732V971567\", # Missing: 2017-mid 2017\n",
    "\"21405V2607269\", # Missing: 2017-early 2019 NB!!!!!!!!\n",
    "\"09269V971425\", # Missing: 2017-mid/late 2017\n",
    "\"52209V971422\", # Missing 2017-late 2017    Prestbakke, litt tvilsom\n",
    "\"02535V971411\", # Missing: 2017-late 2018\n",
    "#\"57474V971423\", # Missing: 2017-late 2018 + sporadisk rundt aarsskiftene.   Berby, bittelitt tvilsom, men kanskje ikke\n",
    "\"04904V971774\", # OK\n",
    "\"35229V971507\"] # OK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298ef0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dates = {\"01777V885181\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)],\n",
    "\"77275V885276\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "#\"51812V1203972\":[datetime.date(2017, 1, 1),datetime.date(2018,12,31)], # Missing: a bit 2018 and early 2019-late 2020. No good data to fit\n",
    "\"35829V885266\": [datetime.date(2018, 1, 15),datetime.date(2020,3,12)], # Missing: 2017-2018\n",
    "\"99923V578123\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "#\"98823V578220\": [datetime.date(2019, 7, 1),datetime.date(2020,1,1)], # Missing: 2017-mid 2019 and early 2020 and a bit 2022.    Røyrvik, litt tvilsom. Not much data pre-pandemic\n",
    "#\"93561V578187\": [datetime.date(2019, 7, 1),datetime.date(2020,3,12)], # Missing: 2017-mid 2019 and a bit late 2019. No good data to fit\n",
    "\"50089V578151\": [datetime.date(2019, 1, 1),datetime.date(2019,12,31)], # Missing: 2017-late 2018 (+ start of pandemic?) ??????????????????\n",
    "\"84237V578097\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "\"76778V704564\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK  Morokulien, litt tvilsom\n",
    "\"69140V704643\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "\"57929V705247\": [datetime.date(2017, 1, 1),datetime.date(2018,5,30)], # Missing: A bit 2018/1019     Øyermoen, litt tvilsom\n",
    "\"52209V971422\": [datetime.date(2018, 1, 1),datetime.date(2020,3,12)], # Missing 2017-late 2017    Prestbakke, litt tvilsom\n",
    "#\"14158V705081\": [datetime.date(2019, 2, 1),datetime.date(2020,3,12)], # Missing: 2017-2019 + sporadisk. Flermoberget, litt tvilsom ??????????\n",
    "#\"08581V885541\": [datetime.date(2019, 9, 1),datetime.date(2020,3,12)], # # Missing: 2017-mid 2019. Too little data for model fit? Ikke pre-pandemic data\n",
    "#\"11051V704737\": [datetime.date(2017, 8, 1),datetime.date(2020,3,12)], # Missing: partly 2017 and 2021 - end. Drevsjø øst, litt tvilsom. Ikke post-pandemic data\n",
    "\"00737V704646\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "\"94864V704707\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # Missing: sporadisk gjennom pandemien?????\n",
    "\"94299V704696\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "\"05732V971567\": [datetime.date(2017, 10, 1),datetime.date(2020,3,12)], # Missing: 2017-mid 2017\n",
    "\"21405V2607269\": [datetime.date(2019, 3, 1),datetime.date(2020,3,12)], # Missing: 2017-early 2019 NB!!!!!!!!\n",
    "\"09269V971425\": [datetime.date(2018, 3, 1),datetime.date(2020,3,12)], # Missing: 2017-mid/late 2017\n",
    "\"02535V971411\": [datetime.date(2019, 1, 1),datetime.date(2020,3,12)], # Missing: 2017-late 2018\n",
    "#\"57474V971423\": [datetime.date(2018, 11, 1),datetime.date(2019,9,30)], # Missing: 2017-late 2018 + sporadisk rundt aarsskiftene.   Berby, bittelitt tvilsom, men kanskje ikke\n",
    "\"04904V971774\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)], # OK\n",
    "\"35229V971507\": [datetime.date(2017, 1, 1),datetime.date(2020,3,12)] } # OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a518567",
   "metadata": {},
   "source": [
    "Read data for each sensor, from .csv files downloaded using the function 'get_traffic_NOR()' in the file road_traffic_data_methodologyPaper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e1a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensor_dict = {}\n",
    "all_data = []\n",
    "for s in Sensors:\n",
    "    sensor_data = pd.read_csv(FolderPath+s+'_by_length_hour.csv', sep=',',usecols= ['sensor_id', 'from_date', 'to_date', 'from_hour','to_hour', 'sensor_dir', 'short_vehicles', 'long_vehicles', 'unknown_length'], parse_dates=['from_date','to_date'])\n",
    "    min_date = sensor_data.from_date.min()\n",
    "    max_date = sensor_data.to_date.max()\n",
    "    sensor_directions = sensor_data.sensor_dir.unique()\n",
    "    sensor_dict[s] = {'Start' : min_date, 'End' : max_date, 'Directions' : sensor_directions}\n",
    "\n",
    "    all_data.append(sensor_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa9142",
   "metadata": {},
   "source": [
    "Change sensor direction to NOR and SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7007afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_norway = ['Trældal x Ev6', 'Hestbrinken', 'Mo i Rana','Trofors','Hattfjelldalen', 'Gjersvika','Sandvika','Nordli','Verdalsøra','Meråker','Drevsjø','ØSTBY','X/RV 25','NYBERGSUND','Holtet','Røgden','ØYERMOEN XF202','KONGSVINGER','BEKKENGA','Oslo','Halden','HALDEN','OSLO']\n",
    "\n",
    "for df in all_data:\n",
    "    df['sensor_dir'] = np.where(list(i in to_norway for i in df['sensor_dir']), 'NOR','SWE')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf5946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_APIdata_NORSWE(data,WhichVehicles,min_date = datetime.datetime(2017, 1, 1)  , max_date = datetime.datetime(2023, 5, 22)):\n",
    "\n",
    "#WhichVehicles can be 'Small', 'Heavy', 'Total' or 'Both' \n",
    "\n",
    "    # What vehicle lengths do we want\n",
    "    if WhichVehicles == 'Total':\n",
    "        data['total_vehicles'] = data['short_vehicles'] + data['long_vehicles'] + data['unknown_length']\n",
    "    elif WhichVehicles == 'Small':\n",
    "        data = data.rename(columns={'short_vehicles':'total_vehicles'})\n",
    "    elif WhichVehicles == 'Heavy':\n",
    "        data = data.rename(columns={'long_vehicles': 'total_vehicles'})\n",
    "\n",
    "    # Create sensor direction and origin columns\n",
    "    data = data.rename(columns = {'sensor_dir': 'dest_country'})\n",
    "    data['origin_country'] = np.where(data['dest_country'] == 'NOR', 'SWE','NOR')\n",
    "\n",
    "    data['sensor_id'] = data['sensor_id'].astype(str)\n",
    "\n",
    "    data['sensor_origin'] = data[['sensor_id', 'origin_country']].agg(', '.join, axis=1)\n",
    "    data['sensor_destination'] = data[['sensor_id', 'dest_country']].agg(', '.join, axis=1)\n",
    "\n",
    "    ## Add some temporal informaiton. \n",
    "\n",
    "    data = data.drop(['to_date'], axis = 1).rename(columns = {'from_date':'date'})\n",
    "\n",
    "    data['minute'] = datetime.timedelta(minutes = 0)\n",
    "\n",
    "\n",
    "    data = data[~(data['from_hour'] == data['to_hour'])].rename(columns = {'from_hour':'hour'})\n",
    "\n",
    "    data['hour'] = pd.to_timedelta(data['hour'].apply(lambda x: int(x[:2])), unit='h')\n",
    "\n",
    "    data['date'] = data['date'] + data['hour'] + data['minute']\n",
    "    data = data[(data.date > min_date) & (data.date < max_date)].copy()\n",
    "\n",
    "    if WhichVehicles == 'Both':\n",
    "        data = data[['sensor_origin','sensor_destination','date','small_vehicles','long_vehicles','unknown_length']].copy()\n",
    "\n",
    "    else:\n",
    "        data = data[['sensor_origin', 'sensor_destination', 'date', 'total_vehicles']].copy().reset_index()\n",
    "        rm_idx = np.where(np.isnan(data.total_vehicles))\n",
    "        data = data.drop(index = rm_idx[0])\n",
    "        data['total_vehicles'] = data['total_vehicles'].apply(lambda x: int(x))\n",
    "\n",
    "        f = lambda x: x.reindex(pd.date_range(min_date,\n",
    "                                                max_date,\n",
    "                                                name='date',\n",
    "                                                freq='1h'), fill_value=0)\n",
    "\n",
    "\n",
    "        data = (data.set_index('date')\n",
    "                    .groupby([\"sensor_origin\", \"sensor_destination\"])[\"total_vehicles\"]\n",
    "                    .apply(f)\n",
    "                    .reset_index())\n",
    "        data = data.pivot_table(index=[\"sensor_origin\",\"sensor_destination\"], columns=[\"date\"],values=[\"total_vehicles\"] ).droplevel(level = 0,axis = 1 )\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55c2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For 2017-2023 this takes about 2 min to run\n",
    "agg_data = []\n",
    "for df in all_data:\n",
    "    d = agg_APIdata_NORSWE(df, 'Small',min_date = datetime.datetime(2017, 1, 1)  , max_date = datetime.datetime(2023, 12, 31))\n",
    "    agg_data.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96293b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n",
      "c:\\Users\\hildlars\\OneDrive - Universitetet i Oslo\\NordicMathCovid\\PanNordicMobility\\FinalFiles_commutingPaper\\Code\\utils.py:122: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  models = data[c].reset_index(\"weekday\").groupby(\"weekday\").agg(lambda x : fit_model(x,hourly_data, nSamp, N = N, hourminute = True,seed = seed, FitMethod = FitMethod))\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "data = []\n",
    "\n",
    "for df in agg_data:\n",
    "    sens = df.index[0][0].split(',')[0]\n",
    "    d1 = fit_dates[sens][0]\n",
    "    d2 = fit_dates[sens][1]\n",
    "    mod, dat = utils.fit_period(df, d1=d1, d2=d2, hourly = True, nSamp = 10000, Normalize = False, N = 10, seed=2024, FitMethod = 'Bayesian')\n",
    "    models.append(mod)\n",
    "    data.append(dat) \n",
    "\n",
    "models = pd.concat(models,axis=1)\n",
    "data = pd.concat(data,axis=0)\n",
    "agg_data = pd.concat(agg_data,axis=0)\n",
    "\n",
    "# Save the fitted model and data.\n",
    "models.to_pickle('../Data/models_nor.pkl')\n",
    "data.to_pickle('../Data/data_nor.pkl')\n",
    "agg_data.to_pickle('../Data/agg_data_nor.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "142fce53b6f2b91b97dca80dfeb7f3677964d22b620fffd446cffe90630072c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

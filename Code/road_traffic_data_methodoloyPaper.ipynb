{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road traffic data from Norway and Finland\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "#import json\n",
    "#import zipfile\n",
    "import datetime\n",
    "import time\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_traffic_nor = 'https://www.vegvesen.no/trafikkdata/api/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sensors in API: 6322\n"
     ]
    }
   ],
   "source": [
    "grap_ql_body = \"\"\"\n",
    "{\n",
    "  trafficRegistrationPoints {\n",
    "    id\n",
    "    name\n",
    "    direction {\n",
    "      from\n",
    "      to\n",
    "    }\n",
    "    location {\n",
    "      coordinates {\n",
    "        latLon {\n",
    "          lat\n",
    "          lon\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "payload = {\n",
    "    'query': grap_ql_body\n",
    "}\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "r = requests.post(url_traffic_nor, json=payload, headers=headers, allow_redirects=True)\n",
    "all_nor_sensors = r.json()\n",
    "all_nor_sensors = all_nor_sensors['data']['trafficRegistrationPoints']\n",
    "print('Number of sensors in API: %s' % (len(all_nor_sensors), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the sensor data from Vegvesenet's API (Norway)\n",
    "\n",
    "The API only allows to get the data sensor by sensor. The following function is in charge of getting the data of a single sensor between the dates `from_day` and `to_day`. Something to note is that the API restricts the size its reponse, i.e. for a sufficiently large time period we would need to make several queries to get the complete data, the data is paged. This function also takes care of that by being recursive and taking the variable `cursor`. This variable is a pointer for the next chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traffic_NOR(sensor_id, from_day, to_day, cursor='', print_errors=True ):\n",
    "\n",
    "    # The below function reads the data from the API\n",
    "    def sensor_traffic_vegvesenetAPI(sensor_id, from_day, to_day, cursor, print_errors):\n",
    "        \n",
    "        url_traffic_nor = 'https://www.vegvesen.no/trafikkdata/api/'\n",
    "        \n",
    "        grap_ql_body = \"\"\"\n",
    "        {{\n",
    "            trafficData(trafficRegistrationPointId: \"{sensor_id}\") {{\n",
    "                volume {{\n",
    "                    byHour(\n",
    "                        from: \"{from_day}T00:00:00+00:00\"\n",
    "                        to: \"{to_day}T00:00:00+00:00\"\n",
    "                        {cursor}\n",
    "                    ) {{\n",
    "                        pageInfo {{\n",
    "                            endCursor\n",
    "                            hasNextPage\n",
    "                        }}\n",
    "                        edges {{\n",
    "                            node {{\n",
    "                                from\n",
    "                                to\n",
    "                                byDirection {{\n",
    "                                    heading\n",
    "                                    byLengthRange {{\n",
    "                                    total {{\n",
    "                                        volumeNumbers {{\n",
    "                                            volume\n",
    "                                        }}\n",
    "                                        coverage {{\n",
    "                                            percentage\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    lengthRange {{\n",
    "                                        representation\n",
    "                                    }}\n",
    "                                    \n",
    "                                }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\".format(\n",
    "            sensor_id=sensor_id,\n",
    "            from_day=from_day,\n",
    "            to_day=to_day,\n",
    "            cursor=cursor\n",
    "        )\n",
    "        payload = {\n",
    "            'query': grap_ql_body\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "        # If we make a lot of requests the API might fail and timeout\n",
    "        # Try to get the information 3 times before quiting\n",
    "        it = 0\n",
    "        r = None\n",
    "        while it < 3:\n",
    "            try:\n",
    "                r = requests.post(url_traffic_nor, json=payload, headers=headers, allow_redirects=True, timeout=10)\n",
    "            except Exception:\n",
    "                if print_errors:\n",
    "                    print(\"TimeoutError: {id}\".format(id=sensor_id))\n",
    "\n",
    "            if r is None:\n",
    "                print(\"Waiting for 5 seconds\")\n",
    "                time.sleep(5)\n",
    "                it += 1\n",
    "            else:\n",
    "                if r.status_code == 200:\n",
    "                    break\n",
    "                elif r.status_code == 404:\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Waiting: {r.status_code}\")\n",
    "                    time.sleep(5)\n",
    "                    it += 1\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            raise ValueError('Incorrect response from API:\\n%s' % (r.content, ))\n",
    "        \n",
    "        sensor_response = r.json()\n",
    "        if sensor_response['data'] is None:\n",
    "            raise ValueError('Incorrect response from API:\\n%s' % (sensor_response, ))\n",
    "\n",
    "        has_next_page = sensor_response['data']['trafficData']['volume']['byHour']['pageInfo']['hasNextPage']\n",
    "        sensor_data = sensor_response['data']['trafficData']['volume']['byHour']['edges']\n",
    "\n",
    "        lines = []\n",
    "        for node_data in r.json()['data']['trafficData']['volume']['byHour']['edges']:\n",
    "            from_date = datetime.datetime.strptime(node_data['node']['from'].split('T')[0],'%Y-%m-%d')\n",
    "            to_date = datetime.datetime.strptime(node_data['node']['to'].split('T')[0],'%Y-%m-%d')\n",
    "            #to_date = node_data['node']['to'].split('T')[0]\n",
    "\n",
    "            from_hour = datetime.time(hour = int(node_data['node']['from'].split('T')[1].split(':')[0]))\n",
    "            to_hour = datetime.time(hour = int(node_data['node']['to'].split('T')[1].split(':')[0]))\n",
    "            \n",
    "            #to_hour = node_data['node']['to'].split('T')[1]\n",
    "\n",
    "            #hour = \"%s-%s\" % (from_hour.split(':')[0], to_hour.split(':')[0])\n",
    "\n",
    "            for direction in  node_data['node']['byDirection']:\n",
    "                dir_name = direction['heading']\n",
    "\n",
    "                small = float('NaN')\n",
    "                heavy = float('NaN')\n",
    "                unknown_length = float('NaN')\n",
    "\n",
    "                for length_range in direction['byLengthRange']: #[0:2]:\n",
    "                    veh_type = length_range['lengthRange']['representation']\n",
    "                    if length_range['total']['volumeNumbers'] is None:\n",
    "                        total = 0\n",
    "                        if print_errors:\n",
    "                            print(\n",
    "                                'Sensor error: %s.\\nDate:%s. Hour: %s\\nDirection: %s.' % (\n",
    "                                    sensor_id,\n",
    "                                    from_date,\n",
    "                                    from_hour,\n",
    "                                    direction\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        total = length_range['total']['volumeNumbers']['volume']\n",
    "\n",
    "                    if veh_type == '[..,5.6)':\n",
    "                        small = total\n",
    "                    elif veh_type == '[5.6,..)':\n",
    "                        heavy = total\n",
    "                    else:\n",
    "                        unknown_length = np.nansum([total,unknown_length])\n",
    "                        #print(unknown_length + ' : Does not recognice vehicle length value')\n",
    "                        #raise ValueError('does not recognice length value') #### CONTINUE HERE!!!!\n",
    "\n",
    "                line = [sensor_id, from_date, to_date, from_hour, to_hour, dir_name, small, heavy, unknown_length]\n",
    "                lines.append(line)\n",
    "\n",
    "        end_cursor = ''\n",
    "\n",
    "        if has_next_page:\n",
    "            end_cursor = sensor_response['data']['trafficData']['volume']['byHour']['pageInfo']['endCursor']\n",
    "        \n",
    "        return has_next_page,end_cursor,lines\n",
    "    \n",
    "\n",
    "    # Read data for a singel sensor\n",
    "    next_page, end_curs, lines = sensor_traffic_vegvesenetAPI(sensor_id, from_day, to_day, cursor, print_errors)\n",
    "\n",
    "    # If the long time period, use the end cursor from last call to read next page  \n",
    "    while next_page: \n",
    "        next_page, end_curs, new_lines = sensor_traffic_vegvesenetAPI(sensor_id, from_day, to_day, cursor='after:\"{end_cursor}\"'.format(end_cursor=end_curs), print_errors=True)\n",
    "        lines = lines + new_lines\n",
    "\n",
    "    # Construct a pandas DF with the data\n",
    "    sensor_traffic_df = pd.DataFrame.from_records(\n",
    "                            lines,\n",
    "                            columns=['sensor_id', 'from_date', 'to_date', 'from_hour','to_hour', 'sensor_dir', 'short_vehicles', 'long_vehicles', 'unknown_length']\n",
    "                        )\n",
    "\n",
    "    return(sensor_traffic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and save traffic for NOR-SWE sensors\n",
    "The data are saved in separate files for each sensor location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutError: 01777V885181\n",
      "Waiting for 5 seconds\n",
      "TimeoutError: 57929V705247\n",
      "Waiting for 5 seconds\n",
      "TimeoutError: 35229V971507\n",
      "Waiting for 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Takes a bit more than an hour to run this\n",
    "\n",
    "sensor_list = [\"01777V885181\", # OK\n",
    "\"77275V885276\", # OK\n",
    "#\"51812V1203972\", # Missing: a bit 2018 and early 2019-late 2020. Wount capture pandemic change with this\n",
    "\"35829V885266\", # Missing: 2017-2018\n",
    "#\"08581V885541\", # Missing: 2017-mid 2019. Too little data for model fit, but include later? Not a lot of data, skip this one.\n",
    "#\"98823V578220\", # Missing: 2017-mid 2019 and early 2020 and a bit 2022    Røyrvik, litt tvilsom\n",
    "\"99923V578123\", # OK\n",
    "#\"93561V578187\", # Missing: 2017-mid 2019 and a bit late 2019. Cannot compare before and after pandemic\n",
    "\"50089V578151\", # Missing: 2017-late 2018 (+ start of pandemic?). ?????????\n",
    "\"84237V578097\", # OK\n",
    "#\"11051V704737\", # Missing: partly 2017 and 2021 - end. Drevsjø øst, litt tvilsom. Ikke post-pandemic data\n",
    "\"69140V704643\", # OK\n",
    "#\"14158V705081\", # Missing: 2017-2019 + sporadisk. Flermoberget, litt tvilsom ??????????\n",
    "\"00737V704646\", # OK\n",
    "\"94864V704707\", # Missing: sporadisk gjennom pandemien ????????\n",
    "\"94299V704696\", # OK\n",
    "\"57929V705247\", # Missing: A bit 2018/1019     Øyermoen, litt tvilsom\n",
    "\"76778V704564\", # OK  Morokulien, litt tvilsom\n",
    "\"05732V971567\", # Missing: 2017-mid 2017\n",
    "\"21405V2607269\", # Missing: 2017-early 2019 NB!!!!!!!!\n",
    "\"09269V971425\", # Missing: 2017-mid/late 2017\n",
    "\"52209V971422\", # Missing 2017-late 2017    Prestbakke, litt tvilsom\n",
    "\"02535V971411\", # Missing: 2017-late 2018\n",
    "#\"57474V971423\", # Missing: 2017-late 2018 + sporadisk rundt aarsskiftene.   Berby, bittelitt tvilsom, men kanskje ikke\n",
    "\"04904V971774\", # OK\n",
    "\"35229V971507\"] # OK\n",
    "\n",
    "from_day = '2017-01-01'\n",
    "to_day = '2023-12-31'\n",
    "\n",
    "path = '../Data/NorSwe/'\n",
    "\n",
    "for s in sensor_list:\n",
    "    d = get_traffic_NOR(s, from_day, to_day, cursor='', print_errors=True )\n",
    "    d.to_csv(path+s+'_by_length_hour.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sensor traffic for Finland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traffic_FIN(start_date, end_date,tmsid):\n",
    "    \n",
    "    #tmsid haparanda: 1431\n",
    "    dates = pd.date_range(start_date,end_date)\n",
    "\n",
    "    df = pd.DataFrame({'date':dates})\n",
    "    #df['year'] = df['date'].apply(lambda x: x.year)\n",
    "\n",
    "    df['days'] = df.apply(lambda x: (x.date-datetime.datetime(int(x.date.year),1,1)).days +1, axis=1)\n",
    "    df['year'] = df.apply(lambda x: int(x.date.strftime(format = '%y')),axis = 1)\n",
    "\n",
    "    col_names = [\"TMS point id\",\"year\",\"ordinal date\",\"hour\",\"minute\",\"second\",\"1/100 second\",\"length (m)\",\"lane\",\"direction\",\"vehicle class\",\"speed (km/h)\",\"faulty (0=valid record, 1=faulty record)\",\"total time (technical)\",\"time interval (technical)\",\"queue start (technical)\"]\n",
    "\n",
    "    data = []\n",
    "    for d, y in zip(df.days,df.year):\n",
    "        url = \"https://tie.digitraffic.fi/api/tms/v1/history/raw/lamraw_\"+ tmsid +\"_\"+str(y)+\"_\"+str(d)+\".csv\"\n",
    "        try:\n",
    "            data_new = pd.read_csv(url,sep = \";\", header = None, names =  col_names).drop(columns=[\"second\",\"1/100 second\",\"vehicle class\",\"speed (km/h)\",\"faulty (0=valid record, 1=faulty record)\",\"total time (technical)\",\"time interval (technical)\",\"queue start (technical)\"])\n",
    "        except:\n",
    "            print('Error:' + url)\n",
    "        else:\n",
    "            data.append(data_new)\n",
    "            #print(url)\n",
    "        \n",
    "\n",
    "    data_df = pd.concat(data)\n",
    "\n",
    "    data_df[\"v_type\"] = np.where(data_df['length (m)'] < 5.6, '<5.6m','>=5.6m')\n",
    "    data_df['total_vehicles'] = 1\n",
    "    data_df = data_df.groupby(['TMS point id','year','ordinal date','hour','minute','v_type','direction'])['total_vehicles'].count().reset_index()\n",
    "    data_df = pd.merge(data_df.rename(columns = {'ordinal date': 'days'}),df,how = 'left',left_on = ['days','year'], right_on=['days','year'])\n",
    "\n",
    "    return(data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_sensor_list = [\"1433\",\"1432\",\"1435\",\"1436\",\"1431\"]\n",
    "path = '../Data/FinSwe/'\n",
    "from_day = datetime.datetime(2017,1,1)\n",
    "to_day = datetime.datetime(2023,12,31)\n",
    "for tms in fin_sensor_list:\n",
    "    d = get_traffic_FIN(from_day, to_day,tmsid=tms)\n",
    "    d.to_csv(path+tms+'_by_length_minute.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "142fce53b6f2b91b97dca80dfeb7f3677964d22b620fffd446cffe90630072c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
